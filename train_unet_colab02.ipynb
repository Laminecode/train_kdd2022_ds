{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c7b2d497",
      "metadata": {},
      "source": [
        "## 1. Setup & Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb967599",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"âš ï¸ No GPU! Go to Runtime â†’ Change runtime type â†’ GPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "install",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q segmentation-models-pytorch albumentations opencv-python-headless"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mount",
      "metadata": {},
      "source": [
        "## 2. Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mount_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "config",
      "metadata": {},
      "source": [
        "## 3. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "config_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============ CONFIG ============\n",
        "import os\n",
        "\n",
        "# Dataset paths - MASKS ARE ALREADY CREATED IN segmentation2 folder\n",
        "DATASET_ROOT = \"/content/drive/MyDrive/segmentation2\"\n",
        "\n",
        "# Training settings\n",
        "ENCODER = 'resnet34'\n",
        "ENCODER_WEIGHTS = 'imagenet'\n",
        "EPOCHS = 60\n",
        "IMG_SIZE = 640\n",
        "BATCH_SIZE = 8\n",
        "LEARNING_RATE = 0.0001\n",
        "PATIENCE = 20\n",
        "\n",
        "# Class names\n",
        "CLASS_NAMES = [\n",
        "    'background',\n",
        "    'Longitudinal_crack',\n",
        "    'Transverse_crack',\n",
        "    'Alligator_crack',\n",
        "    'Other_damage',\n",
        "    'Pothole'\n",
        "]\n",
        "NUM_CLASSES = len(CLASS_NAMES)\n",
        "\n",
        "# Output directory\n",
        "OUTPUT_DIR = \"/content/unet_outputs\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Model: U-Net with {ENCODER} encoder\")\n",
        "print(f\"Epochs: {EPOCHS}, Batch: {BATCH_SIZE}, ImgSize: {IMG_SIZE}\")\n",
        "print(f\"Classes: {NUM_CLASSES} - {CLASS_NAMES}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49d3b488",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3.1 Set random seed for reproducibility\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "set_seed(42)\n",
        "print(\"Random seed set.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "verify",
      "metadata": {},
      "source": [
        "## 4. Verify Dataset Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "verify_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ðŸ“‚ Checking dataset structure...\\n\")\n",
        "\n",
        "for split in ['train', 'val', 'test']:\n",
        "    images_dir = os.path.join(DATASET_ROOT, split, 'images')\n",
        "    masks_dir = os.path.join(DATASET_ROOT, split, 'masks')\n",
        "    \n",
        "    if os.path.exists(images_dir) and os.path.exists(masks_dir):\n",
        "        n_images = len([f for f in os.listdir(images_dir) if f.endswith(('.jpg', '.png', '.jpeg'))])\n",
        "        n_masks = len([f for f in os.listdir(masks_dir) if f.endswith('.png')])\n",
        "        print(f\"{split.upper()}:\")\n",
        "        print(f\"  Images: {n_images}\")\n",
        "        print(f\"  Masks:  {n_masks}\")\n",
        "        print()\n",
        "    else:\n",
        "        print(f\"âš ï¸ {split.upper()}: NOT FOUND\")\n",
        "        print()\n",
        "\n",
        "print(\"âœ“ Dataset verification complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dataset",
      "metadata": {},
      "source": [
        "## 5. Create Dataset and DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dataset_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "class RoadDamageDataset(Dataset):\n",
        "    def __init__(self, images_dir, masks_dir, transform=None):\n",
        "        self.images_dir = images_dir\n",
        "        self.masks_dir = masks_dir\n",
        "        self.transform = transform\n",
        "        self.images = sorted([f for f in os.listdir(images_dir) if f.endswith(('.jpg', '.png', '.jpeg'))])\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.images[idx]\n",
        "        img_path = os.path.join(self.images_dir, img_name)\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        mask_name = os.path.splitext(img_name)[0] + '.png'\n",
        "        mask_path = os.path.join(self.masks_dir, mask_name)\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "        \n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image, mask=mask)\n",
        "            image = augmented['image']\n",
        "            mask = augmented['mask']\n",
        "        \n",
        "        return image, mask.long()\n",
        "\n",
        "# Transforms\n",
        "train_transform = A.Compose([\n",
        "    A.Resize(IMG_SIZE, IMG_SIZE),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.2),\n",
        "    A.RandomRotate90(p=0.5),\n",
        "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=15, p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.5),\n",
        "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.2),\n",
        "    A.Blur(blur_limit=3, p=0.1),\n",
        "    A.Cutout(num_holes=8, max_h_size=32, max_w_size=32, fill_value=0, p=0.5),\n",
        "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ToTensorV2()\n",
        "], p=1.0)\n",
        "\n",
        "val_transform = A.Compose([\n",
        "    A.Resize(IMG_SIZE, IMG_SIZE),\n",
        "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ToTensorV2()\n",
        "], p=1.0)\n",
        "\n",
        "# Create datasets and loaders\n",
        "train_dataset = RoadDamageDataset(\n",
        "    os.path.join(DATASET_ROOT, 'train', 'images'),\n",
        "    os.path.join(DATASET_ROOT, 'train', 'masks'),\n",
        "    transform=train_transform\n",
        ")\n",
        "\n",
        "val_dataset = RoadDamageDataset(\n",
        "    os.path.join(DATASET_ROOT, 'val', 'images'),\n",
        "    os.path.join(DATASET_ROOT, 'val', 'masks'),\n",
        "    transform=val_transform\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "print(f\"âœ“ Train: {len(train_dataset)} | Val: {len(val_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "model",
      "metadata": {},
      "source": [
        "## 6. Build U-Net Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "model_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(Conv -> BN -> ReLU) * 2\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "        super().__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        \n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    \"\"\"Upscaling then double conv\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, bilinear=False):\n",
        "        super().__init__()\n",
        "        \n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
        "            self.conv = DoubleConv(in_channels, out_channels)\n",
        "    \n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        \n",
        "        # Pad x1 to match x2 size if needed\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                        diffY // 2, diffY - diffY // 2])\n",
        "        \n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    \"\"\"Output convolution\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Manual U-Net Implementation\n",
        "    Standard architecture with 4 encoder and 4 decoder blocks\n",
        "    \"\"\"\n",
        "    def __init__(self, n_channels=3, n_classes=6, bilinear=False):\n",
        "        super(UNet, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.bilinear = bilinear\n",
        "        \n",
        "        # Encoder\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        factor = 2 if bilinear else 1\n",
        "        self.down4 = Down(512, 1024 // factor)\n",
        "        \n",
        "        # Decoder\n",
        "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
        "        self.up2 = Up(512, 256 // factor, bilinear)\n",
        "        self.up3 = Up(256, 128 // factor, bilinear)\n",
        "        self.up4 = Up(128, 64, bilinear)\n",
        "        \n",
        "        # Output\n",
        "        self.outc = OutConv(64, n_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Encoder with skip connections\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        \n",
        "        # Decoder with skip connections\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        \n",
        "        logits = self.outc(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# CREATE MODEL\n",
        "# ============================================================================\n",
        "\n",
        "# Create the model\n",
        "model = UNet(n_channels=3, n_classes=NUM_CLASSES, bilinear=False)\n",
        "\n",
        "# Move to device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"âœ… Manual U-Net created!\")\n",
        "print(f\"   Architecture: Standard U-Net with skip connections\")\n",
        "print(f\"   Input channels: 3 (RGB)\")\n",
        "print(f\"   Output classes: {NUM_CLASSES}\")\n",
        "print(f\"   Total parameters: {total_params:,}\")\n",
        "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"   Device: {device}\")\n",
        "print(f\"   Model size: ~{total_params * 4 / (1024**2):.2f} MB\")\n",
        "\n",
        "# Test forward pass\n",
        "test_input = torch.randn(1, 3, IMG_SIZE, IMG_SIZE).to(device)\n",
        "with torch.no_grad():\n",
        "    test_output = model(test_input)\n",
        "print(f\"\\nâœ… Forward pass test:\")\n",
        "print(f\"   Input shape:  {list(test_input.shape)}\")\n",
        "print(f\"   Output shape: {list(test_output.shape)}\")\n",
        "print(f\"\\nðŸŽ¯ Model ready for training!\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# OPTIONAL: Model Architecture Visualization\n",
        "# ============================================================================\n",
        "\n",
        "def print_architecture(model):\n",
        "    \"\"\"Print detailed layer-by-layer architecture\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"DETAILED ARCHITECTURE\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"\\nðŸ“¥ ENCODER (Contracting Path):\")\n",
        "    print(\"  inc:   DoubleConv(3 â†’ 64)\")\n",
        "    print(\"  down1: MaxPool â†’ DoubleConv(64 â†’ 128)\")\n",
        "    print(\"  down2: MaxPool â†’ DoubleConv(128 â†’ 256)\")\n",
        "    print(\"  down3: MaxPool â†’ DoubleConv(256 â†’ 512)\")\n",
        "    print(\"  down4: MaxPool â†’ DoubleConv(512 â†’ 1024)\")\n",
        "    \n",
        "    print(\"\\nðŸ“¤ DECODER (Expansive Path):\")\n",
        "    print(\"  up1: ConvTranspose(1024 â†’ 512) + Skip(512) â†’ DoubleConv â†’ 512\")\n",
        "    print(\"  up2: ConvTranspose(512 â†’ 256) + Skip(256) â†’ DoubleConv â†’ 256\")\n",
        "    print(\"  up3: ConvTranspose(256 â†’ 128) + Skip(128) â†’ DoubleConv â†’ 128\")\n",
        "    print(\"  up4: ConvTranspose(128 â†’ 64) + Skip(64) â†’ DoubleConv â†’ 64\")\n",
        "    \n",
        "    print(\"\\nðŸŽ¯ OUTPUT:\")\n",
        "    print(f\"  outc: Conv1x1(64 â†’ {model.n_classes})\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "# Uncomment to see detailed architecture\n",
        "# print_architecture(model)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# TIPS FOR TRAINING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ðŸ’¡ TRAINING TIPS\")\n",
        "print(\"=\"*60)\n",
        "print(\"1. This is a standard U-Net from scratch (no pretrained weights)\")\n",
        "print(\"2. It may need more epochs to converge vs transfer learning\")\n",
        "print(\"3. Consider using:\")\n",
        "print(\"   - Data augmentation (already in your pipeline)\")\n",
        "print(\"   - Learning rate scheduling (already configured)\")\n",
        "print(\"   - Mixed precision training (already enabled)\")\n",
        "print(\"4. Monitor validation loss to avoid overfitting\")\n",
        "print(\"5. Expected model performance:\")\n",
        "print(\"   - Should reach ~0.60-0.75 mIoU on road damage dataset\")\n",
        "print(\"   - Training time: ~2-3 hours on T4 GPU for 60 epochs\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "training_setup",
      "metadata": {},
      "source": [
        "## 7. Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "training_setup_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, smooth=1.0):\n",
        "        super().__init__()\n",
        "        self.smooth = smooth\n",
        "    \n",
        "    def forward(self, pred, target):\n",
        "        pred = torch.softmax(pred, dim=1)\n",
        "        target_one_hot = torch.nn.functional.one_hot(target, NUM_CLASSES).permute(0, 3, 1, 2).float()\n",
        "        intersection = (pred * target_one_hot).sum(dim=(2, 3))\n",
        "        union = pred.sum(dim=(2, 3)) + target_one_hot.sum(dim=(2, 3))\n",
        "        dice = (2.0 * intersection + self.smooth) / (union + self.smooth)\n",
        "        return 1 - dice.mean()\n",
        "\n",
        "ce_loss = nn.CrossEntropyLoss()\n",
        "dice_loss = DiceLoss()\n",
        "\n",
        "def combined_loss(pred, target):\n",
        "    return ce_loss(pred, target) + dice_loss(pred, target)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, verbose=True)\n",
        "\n",
        "print(\"âœ“ Loss, optimizer, scheduler ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "metrics",
      "metadata": {},
      "source": [
        "## 8. Metrics Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "metrics_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_iou(pred, target, num_classes):\n",
        "    pred = torch.argmax(pred, dim=1)\n",
        "    ious = []\n",
        "    per_class_iou = []\n",
        "    for cls in range(num_classes):\n",
        "        pred_cls = (pred == cls)\n",
        "        target_cls = (target == cls)\n",
        "        intersection = (pred_cls & target_cls).sum().float()\n",
        "        union = (pred_cls | target_cls).sum().float()\n",
        "        if union > 0:\n",
        "            iou = (intersection / union).item()\n",
        "            ious.append(iou)\n",
        "            per_class_iou.append(iou)\n",
        "        else:\n",
        "            per_class_iou.append(float('nan'))\n",
        "    mean_iou = np.nanmean(per_class_iou) if per_class_iou else 0.0\n",
        "    return mean_iou, per_class_iou\n",
        "\n",
        "def calculate_pixel_accuracy(pred, target):\n",
        "    pred = torch.argmax(pred, dim=1)\n",
        "    correct = (pred == target).sum()\n",
        "    total = target.numel()\n",
        "    return (correct / total).item()\n",
        "\n",
        "print(\"âœ“ Metrics ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "train_loop",
      "metadata": {},
      "source": [
        "## 9. Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "train_loop_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "history = {'train_loss': [], 'val_loss': [], 'val_iou': [], 'val_accuracy': [], 'lr': [], 'val_per_class_iou': []}\n",
        "best_iou = 0.0\n",
        "patience_counter = 0\n",
        "best_model_path = os.path.join(OUTPUT_DIR, 'best_unet_model.pth')\n",
        "last_model_path = os.path.join(OUTPUT_DIR, 'last_unet_model.pth')\n",
        "\n",
        "print(\"ðŸš€ Starting training...\\n\")\n",
        "start_time = time.time()\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
        "    \n",
        "    # Training\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    for images, masks in tqdm(train_loader, desc='Training'):\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        if scaler:\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(images)\n",
        "                loss = combined_loss(outputs, masks)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            outputs = model(images)\n",
        "            loss = combined_loss(outputs, masks)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "    train_loss /= len(train_loader)\n",
        "    \n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = val_iou = val_accuracy = 0.0\n",
        "    val_per_class_iou = np.zeros(NUM_CLASSES)\n",
        "    val_batches = 0\n",
        "    with torch.no_grad():\n",
        "        for images, masks in tqdm(val_loader, desc='Validation'):\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "            if scaler:\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    outputs = model(images)\n",
        "                    val_loss += combined_loss(outputs, masks).item()\n",
        "            else:\n",
        "                outputs = model(images)\n",
        "                val_loss += combined_loss(outputs, masks).item()\n",
        "            mean_iou, per_class_iou = calculate_iou(outputs, masks, NUM_CLASSES)\n",
        "            val_iou += mean_iou\n",
        "            val_accuracy += calculate_pixel_accuracy(outputs, masks)\n",
        "            val_per_class_iou += np.nan_to_num(per_class_iou)\n",
        "            val_batches += 1\n",
        "    \n",
        "    val_loss /= len(val_loader)\n",
        "    val_iou /= len(val_loader)\n",
        "    val_accuracy /= len(val_loader)\n",
        "    val_per_class_iou /= val_batches\n",
        "    \n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_iou'].append(val_iou)\n",
        "    history['val_accuracy'].append(val_accuracy)\n",
        "    history['lr'].append(optimizer.param_groups[0]['lr'])\n",
        "    history['val_per_class_iou'].append(val_per_class_iou.tolist())\n",
        "    \n",
        "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | mIoU: {val_iou:.4f} | Acc: {val_accuracy:.4f}\")\n",
        "    print(f\"Per-class IoU: {val_per_class_iou}\")\n",
        "    \n",
        "    scheduler.step(val_iou)\n",
        "    \n",
        "    # Save last model checkpoint\n",
        "    torch.save({'model': model.state_dict(), 'iou': val_iou, 'history': history}, last_model_path)\n",
        "    \n",
        "    if val_iou > best_iou:\n",
        "        best_iou = val_iou\n",
        "        patience_counter = 0\n",
        "        torch.save({'model': model.state_dict(), 'iou': best_iou, 'history': history}, best_model_path)\n",
        "        print(f\"âœ… Best model saved! mIoU: {best_iou:.4f}\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= PATIENCE:\n",
        "            print(\"Early stopping!\")\n",
        "            break\n",
        "\n",
        "print(f\"\\nðŸŽ‰ Training done in {(time.time()-start_time)/60:.1f} min | Best mIoU: {best_iou:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "backup",
      "metadata": {},
      "source": [
        "## 10. Backup to Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "backup_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "drive_backup = \"/content/drive/MyDrive/unet_models\"\n",
        "os.makedirs(drive_backup, exist_ok=True)\n",
        "shutil.copy(best_model_path, os.path.join(drive_backup, 'unet_best.pth'))\n",
        "print(\"âœ“ Backed up to Drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "plots",
      "metadata": {},
      "source": [
        "## 11. Training Curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "plots_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "axes[0,0].plot(history['train_loss'], label='Train')\n",
        "axes[0,0].plot(history['val_loss'], label='Val')\n",
        "axes[0,0].set_title('Loss')\n",
        "axes[0,0].legend()\n",
        "axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[0,1].plot(history['val_iou'])\n",
        "axes[0,1].axhline(best_iou, color='r', linestyle='--', label=f'Best: {best_iou:.4f}')\n",
        "axes[0,1].set_title('mIoU')\n",
        "axes[0,1].legend()\n",
        "axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1,0].plot(history['val_accuracy'])\n",
        "axes[1,0].set_title('Accuracy')\n",
        "axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1,1].plot(history['lr'])\n",
        "axes[1,1].set_title('Learning Rate')\n",
        "axes[1,1].set_yscale('log')\n",
        "axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, 'training_curves.png'), dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "test",
      "metadata": {},
      "source": [
        "## 12. Test Set Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "test_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "checkpoint = torch.load(best_model_path)\n",
        "model.load_state_dict(checkpoint['model'])\n",
        "model.eval()\n",
        "\n",
        "test_dataset = RoadDamageDataset(\n",
        "    os.path.join(DATASET_ROOT, 'test', 'images'),\n",
        "    os.path.join(DATASET_ROOT, 'test', 'masks'),\n",
        "    transform=val_transform\n",
        ")\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "test_iou = test_acc = 0.0\n",
        "with torch.no_grad():\n",
        "    for images, masks in tqdm(test_loader, desc='Testing'):\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "        outputs = model(images)\n",
        "        test_iou += calculate_iou(outputs, masks, NUM_CLASSES)\n",
        "        test_acc += calculate_pixel_accuracy(outputs, masks)\n",
        "\n",
        "test_iou /= len(test_loader)\n",
        "test_acc /= len(test_loader)\n",
        "\n",
        "print(f\"\\nðŸ“Š Test Results: mIoU={test_iou:.4f} | Accuracy={test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "viz",
      "metadata": {},
      "source": [
        "## 13. Visualize Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "viz_code",
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "COLORS = np.array([[0,0,0], [255,0,0], [0,255,0], [0,0,255], [255,255,0], [255,0,255]], dtype=np.uint8)\n",
        "\n",
        "test_imgs = os.listdir(os.path.join(DATASET_ROOT, 'test', 'images'))\n",
        "samples = random.sample(test_imgs, min(6, len(test_imgs)))\n",
        "\n",
        "fig, axes = plt.subplots(6, 3, figsize=(15, 24))\n",
        "\n",
        "for idx, img_name in enumerate(samples):\n",
        "    img_path = os.path.join(DATASET_ROOT, 'test', 'images', img_name)\n",
        "    image = cv2.imread(img_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    mask_path = os.path.join(DATASET_ROOT, 'test', 'masks', os.path.splitext(img_name)[0] + '.png')\n",
        "    gt_mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "    \n",
        "    aug = val_transform(image=image)\n",
        "    inp = aug['image'].unsqueeze(0).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        pred = torch.argmax(model(inp), dim=1).cpu().numpy()[0]\n",
        "    \n",
        "    img_r = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "    gt_r = cv2.resize(gt_mask, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n",
        "    \n",
        "    axes[idx,0].imshow(img_r)\n",
        "    axes[idx,0].set_title(f'Original\\n{img_name}')\n",
        "    axes[idx,0].axis('off')\n",
        "    \n",
        "    axes[idx,1].imshow(COLORS[gt_r])\n",
        "    axes[idx,1].set_title('Ground Truth')\n",
        "    axes[idx,1].axis('off')\n",
        "    \n",
        "    axes[idx,2].imshow(COLORS[pred])\n",
        "    axes[idx,2].set_title('Prediction')\n",
        "    axes[idx,2].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, 'predictions.png'), dpi=150)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
